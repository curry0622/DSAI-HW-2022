# -*- coding: utf-8 -*-
"""DSAI-HW2-2022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MK7ejCkcRCRbILkHMGT7oAi7CkD717rD

## Import Libraries
"""

import os
import csv
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt

def train_and_predict(train_file_name, test_file_name, output_file_name):
  """## Import Dataset"""

  training_set = pd.read_csv(train_file_name, header=None)
  testing_set = pd.read_csv(test_file_name, header=None)
  training_set.head()
  # open high low close

  open_set = training_set.iloc[:,0:1].values

  plt.rcParams['figure.figsize'] = [17, 3]
  plt.rcParams['figure.dpi'] = 100

  """## Dataloading"""

  def sliding_windows(data, seq_length):
      x = []
      y = []

      for i in range(len(data)-seq_length-1):
          _x = data[i:(i+seq_length)]
          _y = data[i+seq_length]
          x.append(_x)
          y.append(_y)

      return np.array(x),np.array(y)

  def sliding_windows2(data, seq_length):
      x = []
      y = []

      for i in range(len(data)-seq_length):
          _x = data[i:(i+seq_length)]
          x.append(_x)

      return np.array(x)

  sc = MinMaxScaler()
  open_data = sc.fit_transform(open_set)

  seq_length = 5
  pred_length = 1
  x, y = sliding_windows(open_data, seq_length)

  trainX = Variable(torch.Tensor(np.array(x)))
  trainY = Variable(torch.Tensor(np.array(y)))

  """## Model"""

  class LSTM(nn.Module):

      def __init__(self, num_classes, input_size, hidden_size, num_layers):
          super(LSTM, self).__init__()

          self.num_classes = num_classes
          self.num_layers = num_layers
          self.input_size = input_size
          self.hidden_size = hidden_size
          self.seq_length = seq_length

          self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,
                              num_layers=num_layers, batch_first=True)

          self.fc = nn.Linear(hidden_size, num_classes)

      def forward(self, x):
          h_0 = Variable(torch.zeros(
              self.num_layers, x.size(0), self.hidden_size))

          c_0 = Variable(torch.zeros(
              self.num_layers, x.size(0), self.hidden_size))

          # Propagate input through LSTM
          ula, (h_out, _) = self.lstm(x, (h_0, c_0))

          h_out = h_out.view(-1, self.hidden_size)

          out = self.fc(h_out)

          return out

  """## Training"""

  num_epochs = 500
  learning_rate = 0.01

  input_size = 1
  hidden_size = 2
  num_layers = 1

  num_classes = pred_length

  lstm = LSTM(num_classes, input_size, hidden_size, num_layers)

  criterion = torch.nn.MSELoss()    # mean-squared error for regression
  optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)

  train_losses = []
  test_losses = []

  # Train the model
  for epoch in range(num_epochs):
      outputs = lstm(trainX)
      optimizer.zero_grad()

      # obtain the loss function
      loss = criterion(outputs, trainY)

      loss.backward()

      train_losses.append(loss.item())

      optimizer.step()
      if epoch % 10 == 0:
        print(f"[Epoch {epoch}] train loss: {round(loss.item(), 4)}")

  plt.plot(
    train_losses,
    label='Train'
  )
  plt.legend()

  """## Plotting"""

  lstm.eval()
  train_predict = lstm(trainX)

  data_predict = train_predict.data.numpy()
  data_real = trainY.data.numpy()

  data_predict = sc.inverse_transform(data_predict)
  data_real = sc.inverse_transform(data_real)
  actual_delta = data_real - data_predict

  plt.plot(data_real)
  plt.plot(data_predict)

  """## Testing"""

  # concatenate training_set last 5 elements with testing_set
  open_test_set_20 = testing_set.iloc[:,0:1].values
  open_test_set = np.concatenate((open_set[-5:], open_test_set_20), axis=0)
  open_test_data = sc.fit_transform(open_test_set)

  x = sliding_windows2(open_test_data, seq_length)

  testX = Variable(torch.Tensor(np.array(x)))

  lstm.eval()
  pred = lstm(testX)
  pred = pred.data.numpy()
  pred = sc.inverse_transform(pred)
  trend = []
  for i in range(0, 19):
    if(pred[i] - open_test_set_20[i] < 0):
      trend.append('跌')
    else:
      trend.append('漲')
  print('trend', trend)

  curr = 0
  actions = []

  for i in range(0, 19):
    if(trend[i] == '跌'):
      if(curr == 0):
        actions.append(-1)
        curr = -1
      elif(curr == -1):
        actions.append(0)
        curr = -1
      else:
        actions.append(-1)
        curr = 0
    else:
      if(curr == 0):
        actions.append(1)
        curr = 1
      elif(curr == -1):
        actions.append(1)
        curr = 0
      else:
        actions.append(0)
        curr = 1

  print('actions', actions)

  """## Actions to csv"""

  df = pd.DataFrame(actions)
  df.to_csv(output_file_name, index = False, header=None)

  # You can write code above the if-main block.

if __name__ == "__main__":
    # You should not modify this part.
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--training", default="training_data.csv", help="input training data file name")
    parser.add_argument("--testing", default="testing_data.csv", help="input testing data file name")
    parser.add_argument("--output", default="output.csv", help="output file name")
    args = parser.parse_args()

    # The following part is an example.
    # You can modify it at will.
    print(args.training, args.testing, args.output)
    train_and_predict(args.training, args.testing, args.output)
